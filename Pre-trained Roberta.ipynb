{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPTNSAkAbpED1cxvewsB35J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"okYLBl1v7toO","executionInfo":{"status":"ok","timestamp":1732482743420,"user_tz":0,"elapsed":55697,"user":{"displayName":"Saffron Sadiq","userId":"04893826538625408231"}},"outputId":"9329122a-2683-458f-b0a1-cc6bb21d064d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Test Metrics: {'f1': 0.47529503810687507, 'precision': 0.43820686612657417, 'recall': 0.5327868852459016}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-2-6b6afbddf69e>:98: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n","  all_labels = torch.tensor(all_labels)\n","<ipython-input-2-6b6afbddf69e>:99: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  all_predictions = torch.tensor(all_predictions)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the dataset\n","df = pd.read_csv('trump_speeches.csv')  # Change to 'biden_speeches.csv' as needed\n","\n","# Define emotion columns\n","emotion_columns = ['optimistic', 'angry', 'fearful', 'proud', 'empathetic', 'determined', 'critical']\n","\n","# Prepare multi-hot encoded labels\n","df['LabelEncoded'] = df[emotion_columns].astype(int).values.tolist()\n","\n","# Extract raw text and labels\n","texts = df['RawText'].tolist()  # Replace 'RawText' with the actual name of your text column\n","labels = df['LabelEncoded'].tolist()\n","\n","# Split the data into train and test sets\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Define the dataset class\n","class EmotionDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        label = self.labels[index]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_len,\n","            add_special_tokens=True,\n","            return_token_type_ids=False,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.float)  # Use float for multi-label classification\n","        }\n","\n","# Initialize tokenizer and model\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","model = RobertaForSequenceClassification.from_pretrained(\n","    'roberta-base',\n","    num_labels=len(emotion_columns)  # Adjust for multi-label classification\n",")\n","model = model.to(device)\n","\n","# Hyperparameters\n","BATCH_SIZE = 16\n","MAX_LEN = 128\n","\n","# Prepare DataLoader\n","train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n","test_dataset = EmotionDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# Evaluation function\n","def evaluate(model, data_loader):\n","    model.eval()\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            predictions = torch.sigmoid(logits) > 0.5  # Threshold for multi-label classification\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predictions.cpu().numpy())\n","\n","    # Calculate metrics\n","    all_labels = torch.tensor(all_labels)\n","    all_predictions = torch.tensor(all_predictions)\n","\n","    f1 = f1_score(all_labels, all_predictions, average='macro')\n","    precision = precision_score(all_labels, all_predictions, average='macro')\n","    recall = recall_score(all_labels, all_predictions, average='macro')\n","\n","    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","# Evaluate the pre-trained model on the test set\n","test_metrics = evaluate(model, test_loader)\n","print(\"Test Metrics:\", test_metrics)"]},{"cell_type":"code","source":[],"metadata":{"id":"EMGq-AX5BY50"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1732482857875,"user_tz":0,"elapsed":40945,"user":{"displayName":"Saffron Sadiq","userId":"04893826538625408231"}},"outputId":"b63495ab-4ea1-439b-bb93-75dcb54ba118","id":"Mn8v30leBZOd"},"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Test Metrics: {'f1': 0.5484349463434826, 'precision': 0.49973104987034905, 'recall': 0.6423645320197044}\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-3-1b6d4e1dd029>:99: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n","  all_predictions = torch.tensor(all_predictions)\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"]}],"source":["import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from sklearn.metrics import f1_score, precision_score, recall_score\n","from sklearn.model_selection import train_test_split\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Load the dataset\n","df = pd.read_csv('biden_speeches.csv')  # Change to 'biden_speeches.csv' as needed\n","\n","# Define emotion columns\n","emotion_columns = ['optimistic', 'angry', 'fearful', 'proud', 'empathetic', 'determined', 'critical']\n","\n","# Prepare multi-hot encoded labels\n","df['LabelEncoded'] = df[emotion_columns].astype(int).values.tolist()\n","\n","# Extract raw text and labels\n","texts = df['RawText'].tolist()  # Replace 'RawText' with the actual name of your text column\n","labels = df['LabelEncoded'].tolist()\n","\n","# Split the data into train and test sets\n","train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n","\n","# Define the dataset class\n","class EmotionDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, index):\n","        text = self.texts[index]\n","        label = self.labels[index]\n","\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            max_length=self.max_len,\n","            add_special_tokens=True,\n","            return_token_type_ids=False,\n","            padding=\"max_length\",\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors=\"pt\",\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'labels': torch.tensor(label, dtype=torch.float)  # Use float for multi-label classification\n","        }\n","\n","# Initialize tokenizer and model\n","tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n","model = RobertaForSequenceClassification.from_pretrained(\n","    'roberta-base',\n","    num_labels=len(emotion_columns)  # Adjust for multi-label classification\n",")\n","model = model.to(device)\n","\n","# Hyperparameters\n","BATCH_SIZE = 16\n","MAX_LEN = 128\n","\n","# Prepare DataLoader\n","train_dataset = EmotionDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n","test_dataset = EmotionDataset(test_texts, test_labels, tokenizer, MAX_LEN)\n","\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","# Evaluation function\n","def evaluate(model, data_loader):\n","    model.eval()\n","    all_labels = []\n","    all_predictions = []\n","\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            predictions = torch.sigmoid(logits) > 0.5  # Threshold for multi-label classification\n","\n","            all_labels.extend(labels.cpu().numpy())\n","            all_predictions.extend(predictions.cpu().numpy())\n","\n","    # Calculate metrics\n","    all_labels = torch.tensor(all_labels)\n","    all_predictions = torch.tensor(all_predictions)\n","\n","    f1 = f1_score(all_labels, all_predictions, average='macro')\n","    precision = precision_score(all_labels, all_predictions, average='macro')\n","    recall = recall_score(all_labels, all_predictions, average='macro')\n","\n","    return {\"f1\": f1, \"precision\": precision, \"recall\": recall}\n","\n","# Evaluate the pre-trained model on the test set\n","test_metrics = evaluate(model, test_loader)\n","print(\"Test Metrics:\", test_metrics)"]}]}